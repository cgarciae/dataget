{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataget \u00b6 Dataget is an easy to use, framework-agnostic, dataset library that gives you quick access to a collection of Machine Learning datasets through a simple API. Main features: Minimal : Downloads entire datasets with just 1 line of code. Framework Agnostic : Loads data as numpy arrays or pandas dataframes which can be easily used with the majority of Machine Learning frameworks. Transparent : By default stores the data in your current project so you can easily inspect it. Memory Efficient : When a dataset doesn't fit in memory it will return metadata instead so you can iteratively load it. Integrates with Kaggle : Supports loading datasets directly from Kaggle in a variety of formats. Checkout the documentation for the list of available datasets. Getting Started \u00b6 In dataget you just have to do two things: Instantiate a Dataset from our collection. Call the get method to download the data to disk and load it into memory. Both are usually done in one line: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist () . get () This example downloads the MNIST dataset to ./data/image_mnist and loads it as numpy arrays. Kaggle Support \u00b6 Kaggle promotes the use of csv files and dataget loves it! With dataget you can quickly download any dataset from the platform and have immediate access to the data: import dataget df_train , df_test = dataget . kaggle ( dataset = \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) To start using Kaggle datasets just make sure you have properly installed and configured the Kaggle API . In the future we want to expand Kaggle support in the following ways: Be able to load any file that numpy or pandas can read. Have generic support for other types of datasets like images, audio, video, etc. e.g dataget.data.kaggle(..., type=\"image\").get(...) Installation \u00b6 dataget is available at pypi so you can use your favorite package manager. pip \u00b6 pip install dataget pipenv \u00b6 pipenv install pytest poetry \u00b6 poetry add dataget Contributing \u00b6 Adding a new dataset is easy! Read our guide on Creating a Dataset if you are interested in contributing a dataset. License \u00b6 MIT License","title":"Introduction"},{"location":"#dataget","text":"Dataget is an easy to use, framework-agnostic, dataset library that gives you quick access to a collection of Machine Learning datasets through a simple API. Main features: Minimal : Downloads entire datasets with just 1 line of code. Framework Agnostic : Loads data as numpy arrays or pandas dataframes which can be easily used with the majority of Machine Learning frameworks. Transparent : By default stores the data in your current project so you can easily inspect it. Memory Efficient : When a dataset doesn't fit in memory it will return metadata instead so you can iteratively load it. Integrates with Kaggle : Supports loading datasets directly from Kaggle in a variety of formats. Checkout the documentation for the list of available datasets.","title":"Dataget"},{"location":"#getting-started","text":"In dataget you just have to do two things: Instantiate a Dataset from our collection. Call the get method to download the data to disk and load it into memory. Both are usually done in one line: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist () . get () This example downloads the MNIST dataset to ./data/image_mnist and loads it as numpy arrays.","title":"Getting Started"},{"location":"#kaggle-support","text":"Kaggle promotes the use of csv files and dataget loves it! With dataget you can quickly download any dataset from the platform and have immediate access to the data: import dataget df_train , df_test = dataget . kaggle ( dataset = \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) To start using Kaggle datasets just make sure you have properly installed and configured the Kaggle API . In the future we want to expand Kaggle support in the following ways: Be able to load any file that numpy or pandas can read. Have generic support for other types of datasets like images, audio, video, etc. e.g dataget.data.kaggle(..., type=\"image\").get(...)","title":"Kaggle Support"},{"location":"#installation","text":"dataget is available at pypi so you can use your favorite package manager.","title":"Installation"},{"location":"#pip","text":"pip install dataget","title":"pip"},{"location":"#pipenv","text":"pipenv install pytest","title":"pipenv"},{"location":"#poetry","text":"poetry add dataget","title":"poetry"},{"location":"#contributing","text":"Adding a new dataset is easy! Read our guide on Creating a Dataset if you are interested in contributing a dataset.","title":"Contributing"},{"location":"#license","text":"MIT License","title":"License"},{"location":"api-reference/","text":"API Reference \u00b6 Dataset \u00b6 __init__ ( self , path = None , global_cache = False ) \u00b6 Show source code in dataget/dataset.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , path : Path = None , global_cache : bool = False ): \"\"\" By default every dataset is downloaded inside `./data/{dataset_name}` in the current directory, however, you can use the the parameters from the base `dataget.Dataset` class constructor to constrol where the data is stored. Parameters: path: if set defines the exact location where the dataset will be stored. Takes precedence over `global_cache`. global_cache: if `True` the data is downloaded to `~/.dataget/{dataset_name}` instead. Use this to reuse datasets across projects. ### Examples Setting `global_cache=True` on any dataset constructor downloads the data to global folder: ```python dataget.image.mnist(global_cache=True).get() ``` By setting the `path` argument you can specify the exact location for the dataset: ```python dataget.image.mnist(path=\"/my/dataset/path\").get() ``` \"\"\" if path and not isinstance ( path , Path ): path = Path ( path ) if path : pass elif global_cache : path = Path ( \"~\" ) . expanduser () / \".dataget\" / self . name else : path = Path ( \"data\" ) / self . name self . path = path By default every dataset is downloaded inside ./data/{dataset_name} in the current directory, however, you can use the the parameters from the base dataget.Dataset class constructor to constrol where the data is stored. Parameters Name Type Description Default path Path if set defines the exact location where the dataset will be stored. Takes precedence over global_cache . None global_cache bool if True the data is downloaded to ~/.dataget/{dataset_name} instead. Use this to reuse datasets across projects. False Examples \u00b6 Setting global_cache=True on any dataset constructor downloads the data to global folder: dataget . image . mnist ( global_cache = True ) . get () By setting the path argument you can specify the exact location for the dataset: dataget . image . mnist ( path = \"/my/dataset/path\" ) . get () get ( self , clean = False , _debug = False , ** kwargs ) \u00b6 Show source code in dataget/dataset.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def get ( self , clean : bool = False , _debug : bool = False , ** kwargs ): \"\"\" Downloads and load the dataset into memory. Parameters: clean: deletes the dataset folder and forces a new download of the data. kwargs: all keyword arguments are forwarded to the `load` method. Consult the documentation on a specific dataset to see which options are available. \"\"\" if clean or not self . is_valid (): if not _debug : shutil . rmtree ( self . path , ignore_errors = True ) self . path . mkdir ( parents = True ) # get data coro = self . download () if coro is not None : asyncio . run ( coro ) # mark as valid ( self . path / \".valid\" ) . touch () return self . load ( ** kwargs ) Downloads and load the dataset into memory. Parameters Name Type Description Default clean bool deletes the dataset folder and forces a new download of the data. False **kwargs _empty all keyword arguments are forwarded to the load method. Consult the documentation on a specific dataset to see which options are available. {}","title":"API Reference"},{"location":"api-reference/#api-reference","text":"","title":"API Reference"},{"location":"api-reference/#dataget.dataset.Dataset","text":"","title":"Dataset"},{"location":"api-reference/#dataget.dataset.Dataset.__init__","text":"Show source code in dataget/dataset.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , path : Path = None , global_cache : bool = False ): \"\"\" By default every dataset is downloaded inside `./data/{dataset_name}` in the current directory, however, you can use the the parameters from the base `dataget.Dataset` class constructor to constrol where the data is stored. Parameters: path: if set defines the exact location where the dataset will be stored. Takes precedence over `global_cache`. global_cache: if `True` the data is downloaded to `~/.dataget/{dataset_name}` instead. Use this to reuse datasets across projects. ### Examples Setting `global_cache=True` on any dataset constructor downloads the data to global folder: ```python dataget.image.mnist(global_cache=True).get() ``` By setting the `path` argument you can specify the exact location for the dataset: ```python dataget.image.mnist(path=\"/my/dataset/path\").get() ``` \"\"\" if path and not isinstance ( path , Path ): path = Path ( path ) if path : pass elif global_cache : path = Path ( \"~\" ) . expanduser () / \".dataget\" / self . name else : path = Path ( \"data\" ) / self . name self . path = path By default every dataset is downloaded inside ./data/{dataset_name} in the current directory, however, you can use the the parameters from the base dataget.Dataset class constructor to constrol where the data is stored. Parameters Name Type Description Default path Path if set defines the exact location where the dataset will be stored. Takes precedence over global_cache . None global_cache bool if True the data is downloaded to ~/.dataget/{dataset_name} instead. Use this to reuse datasets across projects. False","title":"__init__()"},{"location":"api-reference/#examples","text":"Setting global_cache=True on any dataset constructor downloads the data to global folder: dataget . image . mnist ( global_cache = True ) . get () By setting the path argument you can specify the exact location for the dataset: dataget . image . mnist ( path = \"/my/dataset/path\" ) . get ()","title":"Examples"},{"location":"api-reference/#dataget.dataset.Dataset.get","text":"Show source code in dataget/dataset.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def get ( self , clean : bool = False , _debug : bool = False , ** kwargs ): \"\"\" Downloads and load the dataset into memory. Parameters: clean: deletes the dataset folder and forces a new download of the data. kwargs: all keyword arguments are forwarded to the `load` method. Consult the documentation on a specific dataset to see which options are available. \"\"\" if clean or not self . is_valid (): if not _debug : shutil . rmtree ( self . path , ignore_errors = True ) self . path . mkdir ( parents = True ) # get data coro = self . download () if coro is not None : asyncio . run ( coro ) # mark as valid ( self . path / \".valid\" ) . touch () return self . load ( ** kwargs ) Downloads and load the dataset into memory. Parameters Name Type Description Default clean bool deletes the dataset folder and forces a new download of the data. False **kwargs _empty all keyword arguments are forwarded to the load method. Consult the documentation on a specific dataset to see which options are available. {}","title":"get()"},{"location":"dataset/","text":"Creating a Dataset \u00b6 Creating a new dataset is relatively easy, the Dataset class only defined these 3 abstract methods which you must implement: name : a property that returns the folder name of the dataset e.g. image_mnist . download : a method that downloads the data to disk and possibly perform other tasks such as file extraction, organization, and cleanup. load : the method that loads the data into memory and structures it in the most convenient format for the user. Path The self.path field is a pathlib.Path that tells the dataset where the data should be stored. The get method ensures this path exists before calling download or load ; use this field when implementing these methods. get kwargs \u00b6 The get method will accept **kwargs which it will forward to load . For example: def load ( self , dtype = np . float32 ): # code With this implementation the get method can be called like this: . get ( dtype = np . uint8 ) Template \u00b6 You can use this template to get started. from dataget.dataset import Dataset class some_dataset ( Dataset ): # OPTIONAL def __init__ ( self , init_arg , ** kwargs ): # code super () . __init__ ( ** kwargs ) # !!IMPORTANT @property def name ( self ): return \" {dataset_type} _ {dataset_name} \" def download ( self ): # code def load ( self , some_arg ): # code return a , b , c , ... Warning If you are defining your own __init__ remenber to always forward **kwargs to super().__init__ since its very important that all datasets support the path and global_cache keyword arguments defined in the Dataset class. If super().__init__ is not called at all the path field will not be instantiated and errors will occure.","title":"Creating a Dataset"},{"location":"dataset/#creating-a-dataset","text":"Creating a new dataset is relatively easy, the Dataset class only defined these 3 abstract methods which you must implement: name : a property that returns the folder name of the dataset e.g. image_mnist . download : a method that downloads the data to disk and possibly perform other tasks such as file extraction, organization, and cleanup. load : the method that loads the data into memory and structures it in the most convenient format for the user. Path The self.path field is a pathlib.Path that tells the dataset where the data should be stored. The get method ensures this path exists before calling download or load ; use this field when implementing these methods.","title":"Creating a Dataset"},{"location":"dataset/#get-kwargs","text":"The get method will accept **kwargs which it will forward to load . For example: def load ( self , dtype = np . float32 ): # code With this implementation the get method can be called like this: . get ( dtype = np . uint8 )","title":"get kwargs"},{"location":"dataset/#template","text":"You can use this template to get started. from dataget.dataset import Dataset class some_dataset ( Dataset ): # OPTIONAL def __init__ ( self , init_arg , ** kwargs ): # code super () . __init__ ( ** kwargs ) # !!IMPORTANT @property def name ( self ): return \" {dataset_type} _ {dataset_name} \" def download ( self ): # code def load ( self , some_arg ): # code return a , b , c , ... Warning If you are defining your own __init__ remenber to always forward **kwargs to super().__init__ since its very important that all datasets support the path and global_cache keyword arguments defined in the Dataset class. If super().__init__ is not called at all the path field will not be instantiated and errors will occure.","title":"Template"},{"location":"datasets/kaggle/","text":"dataget.kaggle \u00b6 Download any dataset from the Kaggle platform and immediately loads it into memory: import dataget df_train , df_test = dataget . kaggle ( dataset = \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) In this example we downloaded the Point Cloud Mnist 2D dataset from Kaggle and load the train.csv and test.csv files as pandas dataframes. Config To start using this Dataset make sure you have properly installed and configured the Kaggle API . Supported Formats \u00b6 Right now we only support the csv format. In the future we want to be able to load any file that numpy or pandas can read. API Reference \u00b6 kaggle \u00b6 __init__ ( self , dataset = None , competition = None , ** kwargs ) \u00b6 Show source code in kaggle.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , dataset : str = None , competition : str = None , ** kwargs ): \"\"\" Create a Kaggle dataset. You have to specify either `dataset` or `competition`. Arguments: dataset: the id of the kaggle dataset in the format `username/dataset_name`. competition: the name of the kaggle competition. kwargs: common init kwargs. \"\"\" assert ( dataset is not None != competition is not None ), \"Set either dataset or competition\" self . kaggle_dataset = dataset self . kaggle_competition = competition super () . __init__ ( ** kwargs ) Create a Kaggle dataset. You have to specify either dataset or competition . Parameters Name Type Description Default dataset str the id of the kaggle dataset in the format username/dataset_name . None competition str the name of the kaggle competition. None **kwargs _empty common init kwargs. {} load ( self , files ) \u00b6 Show source code in kaggle.py 51 52 53 54 55 56 57 def load ( self , files : list ): \"\"\" Arguments: files: the list of files that will be loaded into memory \"\"\" return [ self . _load_file ( filename ) for filename in files ] Parameters Name Type Description Default files list the list of files that will be loaded into memory required","title":"kaggle"},{"location":"datasets/kaggle/#datagetkaggle","text":"Download any dataset from the Kaggle platform and immediately loads it into memory: import dataget df_train , df_test = dataget . kaggle ( dataset = \"cristiangarcia/pointcloudmnist2d\" ) . get ( files = [ \"train.csv\" , \"test.csv\" ] ) In this example we downloaded the Point Cloud Mnist 2D dataset from Kaggle and load the train.csv and test.csv files as pandas dataframes. Config To start using this Dataset make sure you have properly installed and configured the Kaggle API .","title":"dataget.kaggle"},{"location":"datasets/kaggle/#supported-formats","text":"Right now we only support the csv format. In the future we want to be able to load any file that numpy or pandas can read.","title":"Supported Formats"},{"location":"datasets/kaggle/#api-reference","text":"","title":"API Reference"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle","text":"","title":"kaggle"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle.__init__","text":"Show source code in kaggle.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , dataset : str = None , competition : str = None , ** kwargs ): \"\"\" Create a Kaggle dataset. You have to specify either `dataset` or `competition`. Arguments: dataset: the id of the kaggle dataset in the format `username/dataset_name`. competition: the name of the kaggle competition. kwargs: common init kwargs. \"\"\" assert ( dataset is not None != competition is not None ), \"Set either dataset or competition\" self . kaggle_dataset = dataset self . kaggle_competition = competition super () . __init__ ( ** kwargs ) Create a Kaggle dataset. You have to specify either dataset or competition . Parameters Name Type Description Default dataset str the id of the kaggle dataset in the format username/dataset_name . None competition str the name of the kaggle competition. None **kwargs _empty common init kwargs. {}","title":"__init__()"},{"location":"datasets/kaggle/#dataget.kaggle.kaggle.load","text":"Show source code in kaggle.py 51 52 53 54 55 56 57 def load ( self , files : list ): \"\"\" Arguments: files: the list of files that will be loaded into memory \"\"\" return [ self . _load_file ( filename ) for filename in files ] Parameters Name Type Description Default files list the list of files that will be loaded into memory required","title":"load()"},{"location":"datasets/audio/free_spoken_digit/","text":"dataget.audio.free_spoken_digit \u00b6 Downloads the Free Spoken Digits dataset and loads its metadata as pandas dataframes. The audio samples are as .wav files. import dataget df = dataget . audio . free_spoken_digit () . get () Dataget doesn't load the audio dataset into memory, instead the df dataframe has the audio_path column which contains the relative path of each sample . You can easily load them using scipy.io.wavfile.read . Tip Its recommended that you split train / test based on user instead of randomly to avoid testing based on similar samples found in training. Format \u00b6 type shape df pd.DataFrame (2_000, 4) Features \u00b6 column type description audio_path str Relative path of the audio file label int64 Target label in the range [0, 9] user str Name of the speaker repetition int64 Repetition number for each (user, label) pair, i.e. each user repeats each digit multiple times Info \u00b6 Folder name : audio_free_spoken_digit Size on disk : 26MB","title":"free_spoken_digit"},{"location":"datasets/audio/free_spoken_digit/#datagetaudiofree_spoken_digit","text":"Downloads the Free Spoken Digits dataset and loads its metadata as pandas dataframes. The audio samples are as .wav files. import dataget df = dataget . audio . free_spoken_digit () . get () Dataget doesn't load the audio dataset into memory, instead the df dataframe has the audio_path column which contains the relative path of each sample . You can easily load them using scipy.io.wavfile.read . Tip Its recommended that you split train / test based on user instead of randomly to avoid testing based on similar samples found in training.","title":"dataget.audio.free_spoken_digit"},{"location":"datasets/audio/free_spoken_digit/#format","text":"type shape df pd.DataFrame (2_000, 4)","title":"Format"},{"location":"datasets/audio/free_spoken_digit/#features","text":"column type description audio_path str Relative path of the audio file label int64 Target label in the range [0, 9] user str Name of the speaker repetition int64 Repetition number for each (user, label) pair, i.e. each user repeats each digit multiple times","title":"Features"},{"location":"datasets/audio/free_spoken_digit/#info","text":"Folder name : audio_free_spoken_digit Size on disk : 26MB","title":"Info"},{"location":"datasets/image/cifar10/","text":"dataget.image.cifar10 \u00b6 Downloads the Cifar 10 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . cifar10 () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8 Info \u00b6 Folder name : image_cifar10 Size on disk : 178M","title":"cifar10"},{"location":"datasets/image/cifar10/#datagetimagecifar10","text":"Downloads the Cifar 10 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . cifar10 () . get ()","title":"dataget.image.cifar10"},{"location":"datasets/image/cifar10/#sample","text":"","title":"Sample"},{"location":"datasets/image/cifar10/#format","text":"type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8","title":"Format"},{"location":"datasets/image/cifar10/#info","text":"Folder name : image_cifar10 Size on disk : 178M","title":"Info"},{"location":"datasets/image/cifar100/","text":"dataget.image.cifar100 \u00b6 Downloads the Cifar 100 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . cifar100 () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8 Info \u00b6 Folder name : image_cifar100 Size on disk : 178M","title":"cifar100"},{"location":"datasets/image/cifar100/#datagetimagecifar100","text":"Downloads the Cifar 100 dataset from University of Toronto's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . cifar100 () . get ()","title":"dataget.image.cifar100"},{"location":"datasets/image/cifar100/#sample","text":"","title":"Sample"},{"location":"datasets/image/cifar100/#format","text":"type shape dtype X_train np.array (50_000, 32, 32, 3) uint8 y_train np.array (50_000, 1) uint8 X_test np.array (10_000, 32, 32, 3) uint8 y_test np.array (10_000, 1) uint8","title":"Format"},{"location":"datasets/image/cifar100/#info","text":"Folder name : image_cifar100 Size on disk : 178M","title":"Info"},{"location":"datasets/image/fashion_mnist/","text":"dataget.image.fashion_mnist \u00b6 Downloads the Fashion MNIST dataset and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . fashion_mnist () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8 Info \u00b6 Folder name : image_fashion_mnist Size on disk : 53MB","title":"fashion_mnist"},{"location":"datasets/image/fashion_mnist/#datagetimagefashion_mnist","text":"Downloads the Fashion MNIST dataset and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . fashion_mnist () . get ()","title":"dataget.image.fashion_mnist"},{"location":"datasets/image/fashion_mnist/#sample","text":"","title":"Sample"},{"location":"datasets/image/fashion_mnist/#format","text":"type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8","title":"Format"},{"location":"datasets/image/fashion_mnist/#info","text":"Folder name : image_fashion_mnist Size on disk : 53MB","title":"Info"},{"location":"datasets/image/imagenet/","text":"dataget.image.imagenet \u00b6 Downloads the ImageNet dataset from their official ImageNet Object Localization Challenge Kaggle competition and loads its metadata as pandas dataframes. You need the Kaggle CLI installed and configured to use this dataset. import dataget df_train , df_val , df_test = dataget . image . imagenet () . get () Dataget doesn't load the images of this dataset into memory, instead the df_train , df_val , and df_test dataframes has the image_path column which contains the relative path of each sample which you can latter use to iteratively load each image during training. Sample \u00b6 Format \u00b6 type shape df_train pd.DataFrame (544_546, 10) df_val pd.DataFrame (50_000, 9) df_test pd.DataFrame (100_000, 2) Features \u00b6 column type description df_train df_val df_test ImageId str image id x x x image_path str relative path to jpeg image x x x annotations_path str relative path to pascal voc xml x x label str label id x x label_name str label name x x PredictionString str prediction string x x xmin int64 prediction string bouding box coord x x ymin int64 prediction string bouding box coord x x xmax int64 prediction string bouding box coord x x ymax int64 prediction string bouding box coord x x wnid str WordNet ID x Info \u00b6 Folder name : image_imagenet Size on disk : 161GB","title":"imagenet"},{"location":"datasets/image/imagenet/#datagetimageimagenet","text":"Downloads the ImageNet dataset from their official ImageNet Object Localization Challenge Kaggle competition and loads its metadata as pandas dataframes. You need the Kaggle CLI installed and configured to use this dataset. import dataget df_train , df_val , df_test = dataget . image . imagenet () . get () Dataget doesn't load the images of this dataset into memory, instead the df_train , df_val , and df_test dataframes has the image_path column which contains the relative path of each sample which you can latter use to iteratively load each image during training.","title":"dataget.image.imagenet"},{"location":"datasets/image/imagenet/#sample","text":"","title":"Sample"},{"location":"datasets/image/imagenet/#format","text":"type shape df_train pd.DataFrame (544_546, 10) df_val pd.DataFrame (50_000, 9) df_test pd.DataFrame (100_000, 2)","title":"Format"},{"location":"datasets/image/imagenet/#features","text":"column type description df_train df_val df_test ImageId str image id x x x image_path str relative path to jpeg image x x x annotations_path str relative path to pascal voc xml x x label str label id x x label_name str label name x x PredictionString str prediction string x x xmin int64 prediction string bouding box coord x x ymin int64 prediction string bouding box coord x x xmax int64 prediction string bouding box coord x x ymax int64 prediction string bouding box coord x x wnid str WordNet ID x","title":"Features"},{"location":"datasets/image/imagenet/#info","text":"Folder name : image_imagenet Size on disk : 161GB","title":"Info"},{"location":"datasets/image/mnist/","text":"dataget.image.mnist \u00b6 Downloads the MNIST dataset from Yann LeCun's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . mnist () . get () Sample \u00b6 Format \u00b6 type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8 Info \u00b6 Folder name : image_mnist Size on disk : 53MB","title":"mnist"},{"location":"datasets/image/mnist/#datagetimagemnist","text":"Downloads the MNIST dataset from Yann LeCun's website and loads it as numpy arrays. import dataget X_train , y_train , X_test , y_test = dataget . image . mnist () . get ()","title":"dataget.image.mnist"},{"location":"datasets/image/mnist/#sample","text":"","title":"Sample"},{"location":"datasets/image/mnist/#format","text":"type shape dtype X_train np.array (60_000, 28, 28) uint8 y_train np.array (60_000,) uint8 X_test np.array (10_000, 28, 28) uint8 y_test np.array (10_000,) uint8","title":"Format"},{"location":"datasets/image/mnist/#info","text":"Folder name : image_mnist Size on disk : 53MB","title":"Info"},{"location":"datasets/structured/movielens/movielens_20m/","text":"dataget.structured.movielens_20m \u00b6 Downloads the MovieLens 20M dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_20m () . get () Format \u00b6 type shape ratings pd.DataFrame (20_000_263, 4) movies pd.DataFrame (27_278, 3) tags pd.DataFrame (465_564, 4) links pd.DataFrame (27_278, 3) genome_scores pd.DataFrame (11_709_768, 3) genome_tags pd.DataFrame (1_128, 2) Features \u00b6 ratings \u00b6 column type userId int64 movieId int64 rating float64 timestamp int64 movies \u00b6 column type movieId int64 title object genres object tags \u00b6 column type movieId int64 imdbId int64 tmdbId float64 genome_scores \u00b6 column type movieId int64 tagId int64 relevance float64 genome_tags \u00b6 column type tagId int64 tag object Info \u00b6 Folder name : structured_movielens_20m Size on disk : 836MB","title":"movielens_20m"},{"location":"datasets/structured/movielens/movielens_20m/#datagetstructuredmovielens_20m","text":"Downloads the MovieLens 20M dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_20m () . get ()","title":"dataget.structured.movielens_20m"},{"location":"datasets/structured/movielens/movielens_20m/#format","text":"type shape ratings pd.DataFrame (20_000_263, 4) movies pd.DataFrame (27_278, 3) tags pd.DataFrame (465_564, 4) links pd.DataFrame (27_278, 3) genome_scores pd.DataFrame (11_709_768, 3) genome_tags pd.DataFrame (1_128, 2)","title":"Format"},{"location":"datasets/structured/movielens/movielens_20m/#features","text":"","title":"Features"},{"location":"datasets/structured/movielens/movielens_20m/#ratings","text":"column type userId int64 movieId int64 rating float64 timestamp int64","title":"ratings"},{"location":"datasets/structured/movielens/movielens_20m/#movies","text":"column type movieId int64 title object genres object","title":"movies"},{"location":"datasets/structured/movielens/movielens_20m/#tags","text":"column type movieId int64 imdbId int64 tmdbId float64","title":"tags"},{"location":"datasets/structured/movielens/movielens_20m/#genome_scores","text":"column type movieId int64 tagId int64 relevance float64","title":"genome_scores"},{"location":"datasets/structured/movielens/movielens_20m/#genome_tags","text":"column type tagId int64 tag object","title":"genome_tags"},{"location":"datasets/structured/movielens/movielens_20m/#info","text":"Folder name : structured_movielens_20m Size on disk : 836MB","title":"Info"},{"location":"datasets/structured/movielens/movielens_25m/","text":"dataget.structured.movielens_25m \u00b6 Downloads the MovieLens 25M dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_25m () . get () Format \u00b6 type shape ratings pd.DataFrame (25_000_095, 4) movies pd.DataFrame (62_423, 3) tags pd.DataFrame (1_093_360, 4) links pd.DataFrame (62_423, 3) genome_scores pd.DataFrame (15_584_448, 3) genome_tags pd.DataFrame (1_128, 2) Features \u00b6 ratings \u00b6 column type userId int64 movieId int64 rating float64 timestamp int64 movies \u00b6 column type movieId int64 title object genres object tags \u00b6 column type movieId int64 imdbId int64 tmdbId float64 genome_scores \u00b6 column type movieId int64 tagId int64 relevance float64 genome_tags \u00b6 column type tagId int64 tag object Info \u00b6 Folder name : structured_movielens_25m Size on disk : 1.1GB","title":"movielens_25m"},{"location":"datasets/structured/movielens/movielens_25m/#datagetstructuredmovielens_25m","text":"Downloads the MovieLens 25M dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_25m () . get ()","title":"dataget.structured.movielens_25m"},{"location":"datasets/structured/movielens/movielens_25m/#format","text":"type shape ratings pd.DataFrame (25_000_095, 4) movies pd.DataFrame (62_423, 3) tags pd.DataFrame (1_093_360, 4) links pd.DataFrame (62_423, 3) genome_scores pd.DataFrame (15_584_448, 3) genome_tags pd.DataFrame (1_128, 2)","title":"Format"},{"location":"datasets/structured/movielens/movielens_25m/#features","text":"","title":"Features"},{"location":"datasets/structured/movielens/movielens_25m/#ratings","text":"column type userId int64 movieId int64 rating float64 timestamp int64","title":"ratings"},{"location":"datasets/structured/movielens/movielens_25m/#movies","text":"column type movieId int64 title object genres object","title":"movies"},{"location":"datasets/structured/movielens/movielens_25m/#tags","text":"column type movieId int64 imdbId int64 tmdbId float64","title":"tags"},{"location":"datasets/structured/movielens/movielens_25m/#genome_scores","text":"column type movieId int64 tagId int64 relevance float64","title":"genome_scores"},{"location":"datasets/structured/movielens/movielens_25m/#genome_tags","text":"column type tagId int64 tag object","title":"genome_tags"},{"location":"datasets/structured/movielens/movielens_25m/#info","text":"Folder name : structured_movielens_25m Size on disk : 1.1GB","title":"Info"},{"location":"datasets/structured/movielens/movielens_latest/","text":"dataget.structured.movielens_latest \u00b6 Downloads the MovieLens Latest dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_latest () . get () Warning This dataset is updates automatically. If you need consistency use movielens_25m or movielens_20m instead. Format \u00b6 type shape ratings pd.DataFrame (27_753_444, 4) movies pd.DataFrame (58_098, 3) tags pd.DataFrame (1_108_997, 4) links pd.DataFrame (58_098, 3) genome_scores pd.DataFrame (14_862_528, 3) genome_tags pd.DataFrame (1_128, 2) Features \u00b6 ratings \u00b6 column type userId int64 movieId int64 rating float64 timestamp int64 movies \u00b6 column type movieId int64 title object genres object tags \u00b6 column type movieId int64 imdbId int64 tmdbId float64 genome_scores \u00b6 column type movieId int64 tagId int64 relevance float64 genome_tags \u00b6 column type tagId int64 tag object Info \u00b6 Folder name : structured_movielens_latest Size on disk : 1.2GB","title":"movielens_latest"},{"location":"datasets/structured/movielens/movielens_latest/#datagetstructuredmovielens_latest","text":"Downloads the MovieLens Latest dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , genome_scores , genome_tags , ) = dataget . structured . movielens_latest () . get () Warning This dataset is updates automatically. If you need consistency use movielens_25m or movielens_20m instead.","title":"dataget.structured.movielens_latest"},{"location":"datasets/structured/movielens/movielens_latest/#format","text":"type shape ratings pd.DataFrame (27_753_444, 4) movies pd.DataFrame (58_098, 3) tags pd.DataFrame (1_108_997, 4) links pd.DataFrame (58_098, 3) genome_scores pd.DataFrame (14_862_528, 3) genome_tags pd.DataFrame (1_128, 2)","title":"Format"},{"location":"datasets/structured/movielens/movielens_latest/#features","text":"","title":"Features"},{"location":"datasets/structured/movielens/movielens_latest/#ratings","text":"column type userId int64 movieId int64 rating float64 timestamp int64","title":"ratings"},{"location":"datasets/structured/movielens/movielens_latest/#movies","text":"column type movieId int64 title object genres object","title":"movies"},{"location":"datasets/structured/movielens/movielens_latest/#tags","text":"column type movieId int64 imdbId int64 tmdbId float64","title":"tags"},{"location":"datasets/structured/movielens/movielens_latest/#genome_scores","text":"column type movieId int64 tagId int64 relevance float64","title":"genome_scores"},{"location":"datasets/structured/movielens/movielens_latest/#genome_tags","text":"column type tagId int64 tag object","title":"genome_tags"},{"location":"datasets/structured/movielens/movielens_latest/#info","text":"Folder name : structured_movielens_latest Size on disk : 1.2GB","title":"Info"},{"location":"datasets/structured/movielens/movielens_latest_small/","text":"dataget.structured.movielens_latest_small \u00b6 Downloads the MovieLens Latest Small dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , ) = dataget . structured . movielens_latest_small () . get () Warning This dataset is updates automatically. If you need consistency use movielens_25m or movielens_20m instead. Format \u00b6 type shape ratings pd.DataFrame (100_836, 4) movies pd.DataFrame (9_742, 3) tags pd.DataFrame (3_683, 4) links pd.DataFrame (9_742, 3) Features \u00b6 ratings \u00b6 column type userId int64 movieId int64 rating float64 timestamp int64 movies \u00b6 column type movieId int64 title object genres object tags \u00b6 column type movieId int64 imdbId int64 tmdbId float64 genome_scores \u00b6 column type movieId int64 tagId int64 relevance float64 genome_tags \u00b6 column type tagId int64 tag object Info \u00b6 Folder name : structured_movielens_latest_small Size on disk : 3.2MB","title":"movielens_latest_small"},{"location":"datasets/structured/movielens/movielens_latest_small/#datagetstructuredmovielens_latest_small","text":"Downloads the MovieLens Latest Small dataset and loads it as pandas dataframes. import dataget ( ratings , movies , tags , links , ) = dataget . structured . movielens_latest_small () . get () Warning This dataset is updates automatically. If you need consistency use movielens_25m or movielens_20m instead.","title":"dataget.structured.movielens_latest_small"},{"location":"datasets/structured/movielens/movielens_latest_small/#format","text":"type shape ratings pd.DataFrame (100_836, 4) movies pd.DataFrame (9_742, 3) tags pd.DataFrame (3_683, 4) links pd.DataFrame (9_742, 3)","title":"Format"},{"location":"datasets/structured/movielens/movielens_latest_small/#features","text":"","title":"Features"},{"location":"datasets/structured/movielens/movielens_latest_small/#ratings","text":"column type userId int64 movieId int64 rating float64 timestamp int64","title":"ratings"},{"location":"datasets/structured/movielens/movielens_latest_small/#movies","text":"column type movieId int64 title object genres object","title":"movies"},{"location":"datasets/structured/movielens/movielens_latest_small/#tags","text":"column type movieId int64 imdbId int64 tmdbId float64","title":"tags"},{"location":"datasets/structured/movielens/movielens_latest_small/#genome_scores","text":"column type movieId int64 tagId int64 relevance float64","title":"genome_scores"},{"location":"datasets/structured/movielens/movielens_latest_small/#genome_tags","text":"column type tagId int64 tag object","title":"genome_tags"},{"location":"datasets/structured/movielens/movielens_latest_small/#info","text":"Folder name : structured_movielens_latest_small Size on disk : 3.2MB","title":"Info"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/","text":"dataget.structured.movielens_synthetic_1b \u00b6 Downloads the MovieLens Synthetic 1B dataset returns an iterable of numpy numpy arrays. import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get () By default each array has a different length (the dataset is organized like this), to make training easier we provide the batch_size keyword argument which you can use to get arrays of a given length. import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get ( batch_size = 64 ) Note Depending of the batch_size the last array may have a smaller size than the rest. If you have enough memory you can consider concatenating all into a single array: import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get () ratings = np . concatenate ( ratings_iterable , axis = 0 ) Format \u00b6 type full shape dtype ratings_iterable Iterable[np.array] (1_226_159_268, 2) int64 Features \u00b6 column description total 0 users 22_10_078 1 movies 855_776 Info \u00b6 Folder name : structured_movielens_synthetic_1b Size on disk : 3.1GB","title":"movielens_synthetic_1b"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/#datagetstructuredmovielens_synthetic_1b","text":"Downloads the MovieLens Synthetic 1B dataset returns an iterable of numpy numpy arrays. import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get () By default each array has a different length (the dataset is organized like this), to make training easier we provide the batch_size keyword argument which you can use to get arrays of a given length. import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get ( batch_size = 64 ) Note Depending of the batch_size the last array may have a smaller size than the rest. If you have enough memory you can consider concatenating all into a single array: import dataget ratings_iterable = dataget . structured . movielens_synthetic_1b () . get () ratings = np . concatenate ( ratings_iterable , axis = 0 )","title":"dataget.structured.movielens_synthetic_1b"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/#format","text":"type full shape dtype ratings_iterable Iterable[np.array] (1_226_159_268, 2) int64","title":"Format"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/#features","text":"column description total 0 users 22_10_078 1 movies 855_776","title":"Features"},{"location":"datasets/structured/movielens/movielens_synthetic_1b/#info","text":"Folder name : structured_movielens_synthetic_1b Size on disk : 3.1GB","title":"Info"},{"location":"datasets/text/imdb_reviews/","text":"dataget.text.imdb_reviews \u00b6 Downloads the IMDB Reviews dataset and loads it as pandas dataframes. import dataget df_train , df_test = dataget . text . imdb_reviews () . get () This dataset also contains unsupervised sample, to load them set the include_unsupervised argument: import dataget df_train , df_test = dataget . text . imdb_reviews () . get ( include_unsupervised = True ) All unsupervised sample will have a label of -1 . Format \u00b6 type shape df_train pd.DataFrame (75_000, 3) df_test pd.DataFrame (25_000, 3) Features \u00b6 column type text str label int64 text_path str Info \u00b6 Folder name : text_imdb_reviews Size on disk : 490MB API Reference \u00b6 imdb_reviews \u00b6 load ( self , include_unlabeled = False ) \u00b6 Show source code in imdb_reviews.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def load ( self , include_unlabeled = False ): \"\"\" Arguments: include_unlabeled: whether or not to include the unlabeled samples. \"\"\" train_path = self . path / \"aclImdb\" / \"train\" test_path = self . path / \"aclImdb\" / \"test\" # train df_train = [ self . load_df ( train_path / \"pos\" , label = 1 ), self . load_df ( train_path / \"neg\" , label = 0 ), ] if include_unlabeled : df_train . append ( self . load_df ( train_path / \"unsup\" , label =- 1 )) df_train = pd . concat ( df_train , axis = 0 ) # test df_test = pd . concat ( [ self . load_df ( test_path / \"pos\" , label = 1 ), self . load_df ( test_path / \"neg\" , label = 0 ), ], axis = 0 , ) return df_train , df_test Parameters Name Type Description Default include_unlabeled _empty whether or not to include the unlabeled samples. False","title":"imdb_reviews"},{"location":"datasets/text/imdb_reviews/#datagettextimdb_reviews","text":"Downloads the IMDB Reviews dataset and loads it as pandas dataframes. import dataget df_train , df_test = dataget . text . imdb_reviews () . get () This dataset also contains unsupervised sample, to load them set the include_unsupervised argument: import dataget df_train , df_test = dataget . text . imdb_reviews () . get ( include_unsupervised = True ) All unsupervised sample will have a label of -1 .","title":"dataget.text.imdb_reviews"},{"location":"datasets/text/imdb_reviews/#format","text":"type shape df_train pd.DataFrame (75_000, 3) df_test pd.DataFrame (25_000, 3)","title":"Format"},{"location":"datasets/text/imdb_reviews/#features","text":"column type text str label int64 text_path str","title":"Features"},{"location":"datasets/text/imdb_reviews/#info","text":"Folder name : text_imdb_reviews Size on disk : 490MB","title":"Info"},{"location":"datasets/text/imdb_reviews/#api-reference","text":"","title":"API Reference"},{"location":"datasets/text/imdb_reviews/#dataget.text.imdb_reviews.imdb_reviews","text":"","title":"imdb_reviews"},{"location":"datasets/text/imdb_reviews/#dataget.text.imdb_reviews.imdb_reviews.load","text":"Show source code in imdb_reviews.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def load ( self , include_unlabeled = False ): \"\"\" Arguments: include_unlabeled: whether or not to include the unlabeled samples. \"\"\" train_path = self . path / \"aclImdb\" / \"train\" test_path = self . path / \"aclImdb\" / \"test\" # train df_train = [ self . load_df ( train_path / \"pos\" , label = 1 ), self . load_df ( train_path / \"neg\" , label = 0 ), ] if include_unlabeled : df_train . append ( self . load_df ( train_path / \"unsup\" , label =- 1 )) df_train = pd . concat ( df_train , axis = 0 ) # test df_test = pd . concat ( [ self . load_df ( test_path / \"pos\" , label = 1 ), self . load_df ( test_path / \"neg\" , label = 0 ), ], axis = 0 , ) return df_train , df_test Parameters Name Type Description Default include_unlabeled _empty whether or not to include the unlabeled samples. False","title":"load()"},{"location":"datasets/toy/spirals/","text":"dataget.toy.spirals \u00b6 This is an artificial dataset created using polar functions inspired by a similar dataset found in tensorflow playground . import dataget df_train , df_test = dataget . toy . spirals () . get () Sample \u00b6 Format \u00b6 type shape df_train pd.DataFrame (399, 3) df_test pd.DataFrame (45, 3) Features \u00b6 column type description x0 float coordinate x1 float coordinate y int label Info \u00b6 Folder name : toy_spirals Size on disk : 24KB","title":"spirals"},{"location":"datasets/toy/spirals/#datagettoyspirals","text":"This is an artificial dataset created using polar functions inspired by a similar dataset found in tensorflow playground . import dataget df_train , df_test = dataget . toy . spirals () . get ()","title":"dataget.toy.spirals"},{"location":"datasets/toy/spirals/#sample","text":"","title":"Sample"},{"location":"datasets/toy/spirals/#format","text":"type shape df_train pd.DataFrame (399, 3) df_test pd.DataFrame (45, 3)","title":"Format"},{"location":"datasets/toy/spirals/#features","text":"column type description x0 float coordinate x1 float coordinate y int label","title":"Features"},{"location":"datasets/toy/spirals/#info","text":"Folder name : toy_spirals Size on disk : 24KB","title":"Info"}]}